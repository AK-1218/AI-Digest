# -*- coding: utf-8 -*-
"""AI Feed Generator

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/10ZeqUA9uM-b12OGsgKVfeQ0zsJIL5KAK
"""

# 0. Install required libraries


# 1. Imports
import os
import feedparser
import difflib
import requests
import time
from dateutil import parser as dateparser
from datetime import datetime
from bs4 import BeautifulSoup
from readability import Document
import trafilatura
import google.generativeai as genai
import gspread
from google.oauth2.service_account import Credentials
from gspread_dataframe import set_with_dataframe
import pandas as pd
from concurrent.futures import ThreadPoolExecutor, TimeoutError

# --- after your imports ---
import re, random
from google.api_core.exceptions import ResourceExhausted, DeadlineExceeded, InternalServerError

# 2. Authenticate with Google Sheets (service account; set by GitHub Actions)
SCOPES = [
    "https://www.googleapis.com/auth/spreadsheets",
    "https://www.googleapis.com/auth/drive",  # needed for open(by title)/create/share
]
cred_path = os.environ.get("GOOGLE_APPLICATION_CREDENTIALS", "service_account.json")
creds = Credentials.from_service_account_file(cred_path, scopes=SCOPES)
gc = gspread.authorize(creds)

# Helper: open/create sheet early so we can also read old rows (to reuse summaries)
SHEET_NAME = "AI Digest Sheet"
try:
    sh = gc.open(SHEET_NAME)
except gspread.SpreadsheetNotFound:
    sh = gc.create(SHEET_NAME)
ws = sh.get_worksheet(0) or sh.sheet1

# Reuse summaries we already wrote on previous runs (optional but reduces LLM calls)
try:
    existing = pd.DataFrame(ws.get_all_records()) if ws.acell('A1').value else pd.DataFrame()
    known = {row["Link"]: row.get("Summary", "") for _, row in existing.iterrows()} if not existing.empty else {}
except Exception:
    known = {}

# --- feeds dict stays the same ---

# 6. Init Gemini (use lighter model by default)
genai.configure(api_key=os.environ["GOOGLE_API_KEY"])
MODEL_ID = os.environ.get("GEMINI_MODEL", "gemini-1.5-flash")  # lighter & cheaper than pro
_gem_model = genai.GenerativeModel(MODEL_ID)

def _call_gemini(prompt, max_retries=5, base_delay=25):
    """Robust call with exponential backoff on 429s and transient errors."""
    for attempt in range(max_retries):
        try:
            return _gem_model.generate_content(
                prompt,
                generation_config={"max_output_tokens": 256}
            )
        except (ResourceExhausted, DeadlineExceeded, InternalServerError) as e:
            # backoff with jitter
            delay = base_delay * (2 ** attempt)
            time.sleep(delay + random.uniform(0, 5))
        except Exception as e:
            # non-retryable or unknown; bubble up on last attempt
            if attempt == max_retries - 1:
                raise
            time.sleep(8 + 4 * attempt)

def _naive_fallback(text, n_sentences=3):
    """Free, offline fallback: first N sentences of cleaned text."""
    if not text:
        return ""
    sents = [s.strip() for s in re.split(r'(?<=[.!?])\s+', text) if s.strip()]
    return " ".join(sents[:n_sentences])[:900]

def gemini_summary(text, trim=4500):
    """Summarize with Gemini; keep inputs small; fallback if quota is hit."""
    if not text:
        return ""
    # keep inputs tiny for free-tier quotas
    snippet = text[:trim]
    prompt = (
        "Summarize the following article in EXACTLY 3 sentences.\n"
        "- Keep it factual and concise. Include key numbers if present.\n"
        "- No hype or predictions.\n\n"
        f"{snippet}\n\n"
        "Now write the 3-sentence summary:"
    )
    try:
        resp = _call_gemini(prompt)
        out = (resp.text or "").strip()
        sents = [s.strip() for s in re.split(r'(?<=[.!?])\s+', out) if s.strip()]
        if not sents:
            return _naive_fallback(text)
        return " ".join(sents[:3])
    except Exception:
        # If we still fail (e.g., daily quota), fallback so the pipeline continues
        return _naive_fallback(text)

# --- your fetch/clean loop stays the same up to building all_articles ---

# 5. Keep the 5 newest articles
all_articles = sorted(all_articles, key=lambda x: x["PubDT"], reverse=True)[:5]

# 7. Generate the final “Summary” field (reuse known summaries; otherwise call model)
for i, art in enumerate(all_articles, start=1):
    print(f"[{i}/{len(all_articles)}] Summarizing: {art['Title'][:60]}…")
    if art["Link"] in known and known[art["Link"]]:
        art["Summary"] = known[art["Link"]]
        continue
    art["Summary"] = gemini_summary(art["Content"])
    time.sleep(3)  # gentle pacing for free-tier/minute quotas

# 8–9. Build DF and write to Google Sheet (always execute, even if some summaries failed)
df = pd.DataFrame(all_articles)[["Source", "Date", "Title", "Link", "Summary"]]
ws.clear()
set_with_dataframe(ws, df)
print("✅ AI Digest Sheet updated (with backoff + fallback).")

