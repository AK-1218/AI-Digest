# -*- coding: utf-8 -*-
"""AI Feed Generator

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/10ZeqUA9uM-b12OGsgKVfeQ0zsJIL5KAK
"""

# 0. Install required libraries


# 1. Imports
import os
import feedparser
import difflib
import requests
import time
from dateutil import parser as dateparser
from datetime import datetime
from bs4 import BeautifulSoup
from readability import Document
import trafilatura
import google.generativeai as genai
import gspread
from google.oauth2.service_account import Credentials
from gspread_dataframe import set_with_dataframe
import pandas as pd
from concurrent.futures import ThreadPoolExecutor, TimeoutError

# --- after your imports ---
import re, random
from google.api_core.exceptions import ResourceExhausted, DeadlineExceeded, InternalServerError

# 2. Authenticate with Google Sheets (service account; set by GitHub Actions)
SCOPES = [
    "https://www.googleapis.com/auth/spreadsheets",
    "https://www.googleapis.com/auth/drive",  # needed for open(by title)/create/share
]
cred_path = os.environ.get("GOOGLE_APPLICATION_CREDENTIALS", "service_account.json")
creds = Credentials.from_service_account_file(cred_path, scopes=SCOPES)
gc = gspread.authorize(creds)

# Helper: open/create sheet early so we can also read old rows (to reuse summaries)
SHEET_NAME = "AI Digest Sheet"
try:
    sh = gc.open(SHEET_NAME)
except gspread.SpreadsheetNotFound:
    sh = gc.create(SHEET_NAME)
ws = sh.get_worksheet(0) or sh.sheet1

# Reuse summaries we already wrote on previous runs (optional but reduces LLM calls)
try:
    existing = pd.DataFrame(ws.get_all_records()) if ws.acell('A1').value else pd.DataFrame()
    known = {row["Link"]: row.get("Summary", "") for _, row in existing.iterrows()} if not existing.empty else {}
except Exception:
    known = {}

# --- feeds dict stays the same ---

# 6. Init Gemini (use lighter model by default)
genai.configure(api_key=os.environ["GOOGLE_API_KEY"])
MODEL_ID = os.environ.get("GEMINI_MODEL", "gemini-1.5-flash")  # lighter & cheaper than pro
_gem_model = genai.GenerativeModel(MODEL_ID)

def _call_gemini(prompt, max_retries=5, base_delay=25):
    """Robust call with exponential backoff on 429s and transient errors."""
    for attempt in range(max_retries):
        try:
            return _gem_model.generate_content(
                prompt,
                generation_config={"max_output_tokens": 256}
            )
        except (ResourceExhausted, DeadlineExceeded, InternalServerError) as e:
            # backoff with jitter
            delay = base_delay * (2 ** attempt)
            time.sleep(delay + random.uniform(0, 5))
        except Exception as e:
            # non-retryable or unknown; bubble up on last attempt
            if attempt == max_retries - 1:
                raise
            time.sleep(8 + 4 * attempt)

def _naive_fallback(text, n_sentences=3):
    """Free, offline fallback: first N sentences of cleaned text."""
    if not text:
        return ""
    sents = [s.strip() for s in re.split(r'(?<=[.!?])\s+', text) if s.strip()]
    return " ".join(sents[:n_sentences])[:900]

def gemini_summary(text, trim=4500):
    """Summarize with Gemini; keep inputs small; fallback if quota is hit."""
    if not text:
        return ""
    # keep inputs tiny for free-tier quotas
    snippet = text[:trim]
    prompt = (
        "Summarize the following article in EXACTLY 3 sentences.\n"
        "- Keep it factual and concise. Include key numbers if present.\n"
        "- No hype or predictions.\n\n"
        f"{snippet}\n\n"
        "Now write the 3-sentence summary:"
    )
    try:
        resp = _call_gemini(prompt)
        out = (resp.text or "").strip()
        sents = [s.strip() for s in re.split(r'(?<=[.!?])\s+', out) if s.strip()]
        if not sents:
            return _naive_fallback(text)
        return " ".join(sents[:3])
    except Exception:
        # If we still fail (e.g., daily quota), fallback so the pipeline continues
        return _naive_fallback(text)

# --- your fetch/clean loop stays the same up to building all_articles ---

# 3. RSS feeds & human-readable names (re-add this)
feeds = {
    "https://blog.arxiv.org/feed":               "arXiv Blog",
    "https://ai.meta.com/blog/rss/":             "Meta AI Blog",
    "https://huggingface.co/blog/feed.xml":      "Hugging Face Blog",
    "https://openai.com/blog/rss.xml":           "OpenAI Blog",
    "https://deepmind.google/discover/blog/rss": "DeepMind Blog",
}

# 4. Fetch, dedupe, extract & clean content (re-add this)
all_articles = []
seen_urls = set()
seen_titles = []

UA = {"User-Agent": "Mozilla/5.0"}

for url, src in feeds.items():
    try:
        feed = feedparser.parse(url)
    except Exception:
        continue

    # cap per-feed to be gentle on quotas
    for entry in feed.entries[:10]:
        # 4a) robust link extraction
        link = (entry.get("link") or entry.get("id") or entry.get("guid", "")).strip()
        if not link or link in seen_urls:
            continue

        # 4b) title & near-duplicate title filter
        title = (entry.get("title") or "").strip()
        tl = title.lower()
        if any(difflib.SequenceMatcher(None, tl, other).ratio() > 0.6 for other in seen_titles):
            continue

        seen_urls.add(link)
        seen_titles.append(tl)

        # 4c) publication date
        raw_dt = entry.get("published", entry.get("updated", "")) or ""
        try:
            pub_dt = dateparser.parse(raw_dt)
        except Exception:
            pub_dt = datetime(2000, 1, 1)

        # 4d) choose richest HTML field available
        raw_html = ""
        try:
            if entry.get("content"):
                raw_html = entry.content[0].value
            elif entry.get("summary"):
                raw_html = entry.summary
            elif entry.get("media_content"):
                maybe_url = entry.media_content[0].get("url", "")
                if maybe_url.startswith("http"):
                    raw_html = requests.get(maybe_url, timeout=10, headers=UA).text
        except Exception:
            raw_html = ""

        # 4e) readability to strip boilerplate
        content = ""
        if raw_html.strip():
            try:
                clean_html = Document(raw_html).summary()
                content = BeautifulSoup(clean_html, "html.parser").get_text(" ", strip=True)
            except Exception:
                content = ""

        # 4f) fallback: fetch full page & re-extract if still tiny
        if len(content.split()) < 50:
            try:
                page_html = requests.get(link, timeout=10, headers=UA).text
                clean_page = Document(page_html).summary()
                candidate = BeautifulSoup(clean_page, "html.parser").get_text(" ", strip=True)
                if len(candidate.split()) > len(content.split()):
                    content = candidate
            except Exception:
                pass

        # 4g) append
        all_articles.append({
            "Source":  src,
            "Date":    pub_dt.strftime("%Y-%m-%d"),
            "Title":   title,
            "Link":    link,
            "Content": content,
            "PubDT":   pub_dt,
        })

# Safety: handle empty result to avoid crashes
if not all_articles:
    print("No articles found; writing empty sheet and exiting.")
    ws.clear()
    set_with_dataframe(ws, pd.DataFrame(columns=["Source", "Date", "Title", "Link", "Summary"]))
    raise SystemExit(0)

# 5. Select a 5-item batch with ≥3 sources (if possible), newest first
# ----------------------------
MAX_UPDATES_PER_RUN = 5
MIN_DISTINCT_SOURCES = 3
CANDIDATE_POOL = 60  # look across more items so we can diversify

# sort everything we scraped by recency (desc)
all_articles = sorted(all_articles, key=lambda x: x["PubDT"], reverse=True)

# candidates we haven't posted before (based on Link)
unseen = [a for a in all_articles if a["Link"] not in known][:CANDIDATE_POOL]

if not unseen:
    print("No unseen articles available. Nothing to insert this run.")
    # nothing new; keep sheet untouched and exit cleanly
    raise SystemExit(0)

distinct_sources_available = len({a["Source"] for a in unseen})
need_distinct = min(MIN_DISTINCT_SOURCES, MAX_UPDATES_PER_RUN, distinct_sources_available)

selected = []
used_links = set()
used_sources = set()

# Phase 1: ensure diversity—pick the newest items that add *new* sources
for art in unseen:
    if len(selected) >= MAX_UPDATES_PER_RUN:
        break
    if art["Link"] in used_links:
        continue
    if len(used_sources) < need_distinct:
        if art["Source"] in used_sources:
            continue  # skip for now to increase diversity
        selected.append(art)
        used_links.add(art["Link"])
        used_sources.add(art["Source"])

# Phase 2: fill the remaining slots by pure recency (any source)
if len(selected) < MAX_UPDATES_PER_RUN:
    for art in unseen:
        if len(selected) >= MAX_UPDATES_PER_RUN:
            break
        if art["Link"] in used_links:
            continue
        selected.append(art)
        used_links.add(art["Link"])
        used_sources.add(art["Source"])

print(f"Picked {len(selected)} updates; sources in batch: {sorted(used_sources)}")

# ----------------------------
# 6. Summarize only the selected items (reuse when possible)
# ----------------------------
for i, art in enumerate(selected, start=1):
    print(f"[{i}/{len(selected)}] Summarizing: {art['Title'][:80]}…")
    if art["Link"] in known and known[art["Link"]]:
        art["Summary"] = known[art["Link"]]
        continue
    art["Summary"] = gemini_summary(art["Content"])
    time.sleep(3)  # gentle pacing for free-tier

# ----------------------------
# 7. Prepend new rows to the sheet (don’t clear existing)
# ----------------------------
expected_header = ["Source", "Date", "Title", "Link", "Summary"]
try:
    current_header = [c.strip() for c in ws.row_values(1)[:5]]
except Exception:
    current_header = []

# If header missing/mismatched, set it once
if current_header != expected_header:
    ws.insert_row(expected_header, 1)

# Build rows (keep newest first so row 2 is the most recent)
rows_to_insert = [
    [art["Source"], art["Date"], art["Title"], art["Link"], art["Summary"]]
    for art in selected
]

# Insert all at once at row 2 (directly under header)
# If your gspread version doesn't support insert_rows(list_of_rows),
# you can loop and ws.insert_row(row, 2) in reverse order.
try:
    ws.insert_rows(rows_to_insert, row=2, value_input_option='RAW')
except TypeError:
    # Fallback for older gspread: insert one-by-one in reverse
    for row in reversed(rows_to_insert):
        ws.insert_row(row, 2, value_input_option='RAW')

print("✅ Prepended updates to Google Sheet without replacing older rows.")


