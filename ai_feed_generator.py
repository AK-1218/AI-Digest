# -*- coding: utf-8 -*-
"""AI Feed Generator

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/10ZeqUA9uM-b12OGsgKVfeQ0zsJIL5KAK
"""

# 0. Install required libraries


# 1. Imports
import os
import feedparser
import difflib
import requests
import time
from dateutil import parser as dateparser
from datetime import datetime
from bs4 import BeautifulSoup
from readability import Document
import trafilatura
import google.generativeai as genai
import gspread
from google.oauth2.service_account import Credentials
from gspread_dataframe import set_with_dataframe
import pandas as pd
from concurrent.futures import ThreadPoolExecutor, TimeoutError

# 2. Authenticate with Google Sheets (service account; set by GitHub Actions)
SCOPES = ["https://www.googleapis.com/auth/spreadsheets"]
cred_path = os.environ.get("GOOGLE_APPLICATION_CREDENTIALS", "service_account.json")
creds = Credentials.from_service_account_file(cred_path, scopes=SCOPES)
gc = gspread.authorize(creds)

# 3. RSS feeds & human-readable names
feeds = {
    "https://blog.arxiv.org/feed":              "arXiv Blog",
    "https://ai.meta.com/blog/rss/":            "Meta AI Blog",
    "https://huggingface.co/blog/feed.xml":     "Hugging Face Blog",
    "https://openai.com/blog/rss.xml":          "OpenAI Blog",
    "https://deepmind.google/discover/blog/rss": "DeepMind Blog"
}

# 4. Fetch, dedupe, extract & clean content
all_articles = []
seen_urls = set()
seen_titles = []

for url, src in feeds.items():
    feed = feedparser.parse(url)
    for entry in feed.entries:
        # 4a) robust link extraction
        link = (entry.get("link") or entry.get("id") or entry.get("guid", "")).strip()
        if not link or link in seen_urls:
            continue

        title = entry.get("title", "").strip()
        tl = title.lower()
        if any(difflib.SequenceMatcher(None, tl, other).ratio() > 0.6 for other in seen_titles):
            continue

        seen_urls.add(link)
        seen_titles.append(tl)

        # 4b) publication date
        raw_dt = entry.get("published", entry.get("updated", ""))
        try:
            pub_dt = dateparser.parse(raw_dt)
        except:
            pub_dt = datetime(2000, 1, 1)

        # 4c) pull the richest HTML field available
        raw_html = ""
        if entry.get("content"):
            raw_html = entry.content[0].value
        elif entry.get("summary"):
            raw_html = entry.summary
        elif entry.get("media_content"):
            maybe_url = entry.media_content[0].get("url", "")
            if maybe_url.startswith("http"):
                try:
                    raw_html = requests.get(maybe_url, timeout=10).text
                except requests.exceptions.RequestException:
                    raw_html = ""
        raw_html = raw_html or ""

        # 4d) readability-lxml to strip navigation, ads, boilerplate
        content = ""
        if raw_html.strip():
            try:
                clean_html = Document(raw_html).summary()
                content = BeautifulSoup(clean_html, "html.parser") \
                            .get_text(" ", strip=True)
            except Exception:
                content = ""

        # 4e) fallback: fetch full page & re-extract if still tiny (<50 words)
        if len(content.split()) < 50:
            try:
                page_html = requests.get(link, timeout=10).text
                clean_page = Document(page_html).summary()
                candidate = BeautifulSoup(clean_page, "html.parser") \
                              .get_text(" ", strip=True)
                if len(candidate.split()) > len(content.split()):
                    content = candidate
            except Exception:
                pass  # keep whatever content we have

        # 4f) append to list
        all_articles.append({
            "Source":  src,
            "Date":    pub_dt.strftime("%Y-%m-%d"),
            "Title":   title,
            "Link":    link,
            "Content": content,
            "PubDT":   pub_dt
        })

# 5. Keep the 5 newest articles
all_articles = sorted(all_articles, key=lambda x: x["PubDT"], reverse=True)[:5]

# 6. Init Gemini (reads your key from the GitHub secret)
genai.configure(api_key=os.environ["GOOGLE_API_KEY"])
_gem_model = genai.GenerativeModel("gemini-1.5-pro")

def gemini_summary(text, trim=60000):
    if not text or len(text.split()) < 50:
        return text
    snippet = text[:trim]
    prompt = (
        "Summarize the following article in EXACTLY 3 sentences.\n"
        "- No fluff or hype; keep it factual and clear.\n"
        "- Include key numbers if present.\n"
        "- No opinions or predictions.\n\n"
        f"{snippet}\n\n"
        "Now write the 3-sentence summary:"
    )
    resp = _gem_model.generate_content(prompt)
    out = (resp.text or "").strip()
    # keep exactly 3 sentences
    import re
    sents = [s.strip() for s in re.split(r'(?<=[.!?])\s+', out) if s.strip()]
    return " ".join(sents[:3]) if sents else out

# 7. Generate the final “Summary” field
for i, art in enumerate(all_articles, start=1):
    print(f"[{i}/{len(all_articles)}] Summarizing: {art['Title'][:60]}…")
    art["Summary"] = gemini_summary(art["Content"])
    time.sleep(1)

# 8. Build DataFrame
df = pd.DataFrame(all_articles)[["Source", "Date", "Title", "Link", "Summary"]]

# 9. Write to Google Sheet
sheet_name = "AI Digest Sheet"
try:
    sh = gc.open(sheet_name)
except gspread.SpreadsheetNotFound:
    sh = gc.create(sheet_name)
ws = sh.get_worksheet(0)
ws.clear()
set_with_dataframe(ws, df)

print("✅ AI Digest Sheet updated with cleaner, full-text summaries!")
