# -*- coding: utf-8 -*-
"""AI Feed Generator

Automatically generated by Colab; adapted for GitHub Actions.

- Robust RSS link extraction (handles feeds missing <link>, e.g., HF)
- Reliable pub-date parsing with timezone
- Per-source near-duplicate title filter
- Prioritize last 48h first; ensure ≥3 sources in the daily batch when possible
- Backoff + fallback summaries to survive free-tier quotas
"""

# 0. (packages installed via requirements.txt)

# 1. Imports
import os
import re
import random
import time
import difflib
import requests

from datetime import datetime, timezone, timedelta
from dateutil import parser as dateparser

import feedparser
from bs4 import BeautifulSoup
from readability import Document
import trafilatura  # optional; kept if you want to experiment later

import google.generativeai as genai

import gspread
from google.oauth2.service_account import Credentials
from gspread_dataframe import set_with_dataframe
import pandas as pd
from concurrent.futures import ThreadPoolExecutor, TimeoutError

from google.api_core.exceptions import ResourceExhausted, DeadlineExceeded, InternalServerError

# 2. Authenticate with Google Sheets (service account; set by GitHub Actions)
SCOPES = [
    "https://www.googleapis.com/auth/spreadsheets",
    "https://www.googleapis.com/auth/drive",  # needed for open(by title)/create/share
]
cred_path = os.environ.get("GOOGLE_APPLICATION_CREDENTIALS", "service_account.json")
creds = Credentials.from_service_account_file(cred_path, scopes=SCOPES)
gc = gspread.authorize(creds)

SHEET_NAME = "AI Digest Sheet"
SHEET_ID = os.environ.get("SHEET_ID")  # prefer ID if provided

try:
    if SHEET_ID:
        sh = gc.open_by_key(SHEET_ID)
    else:
        try:
            sh = gc.open(SHEET_NAME)
        except gspread.SpreadsheetNotFound:
            sh = gc.create(SHEET_NAME)
except Exception as e:
    raise RuntimeError(f"Unable to open or create Google Sheet: {e}")

ws = sh.get_worksheet(0) or sh.sheet1

# Reuse summaries we already wrote on previous runs (optional but reduces LLM calls)
try:
    has_header = bool(ws.row_values(1))
except Exception:
    has_header = False

try:
    existing = pd.DataFrame(ws.get_all_records()) if has_header else pd.DataFrame()
    known = {row["Link"]: row.get("Summary", "") for _, row in existing.iterrows()} if not existing.empty else {}
except Exception:
    known = {}

# 3. Init Gemini (use lighter model by default)
genai.configure(api_key=os.environ["GOOGLE_API_KEY"])
MODEL_ID = os.environ.get("GEMINI_MODEL", "gemini-1.5-flash")  # lighter & cheaper than pro
_gem_model = genai.GenerativeModel(MODEL_ID)

def _call_gemini(prompt, max_retries=5, base_delay=25):
    """Robust call with exponential backoff on 429s and transient errors."""
    for attempt in range(max_retries):
        try:
            return _gem_model.generate_content(
                prompt,
                generation_config={"max_output_tokens": 256}
            )
        except (ResourceExhausted, DeadlineExceeded, InternalServerError):
            delay = base_delay * (2 ** attempt)
            time.sleep(delay + random.uniform(0, 5))
        except Exception:
            if attempt == max_retries - 1:
                raise
            time.sleep(8 + 4 * attempt)

def _naive_fallback(text, n_sentences=3):
    """Free, offline fallback: first N sentences of cleaned text."""
    if not text:
        return ""
    sents = [s.strip() for s in re.split(r'(?<=[.!?])\s+', text) if s.strip()]
    return " ".join(sents[:n_sentences])[:900]

def gemini_summary(text, trim=4500):
    """Summarize with Gemini; keep inputs small; fallback if quota is hit."""
    if not text:
        return ""
    snippet = text[:trim]
    prompt = (
        "Summarize the following article in EXACTLY 3 sentences.\n"
        "- Keep it factual and concise. Include key numbers if present.\n"
        "- No hype or predictions.\n\n"
        f"{snippet}\n\n"
        "Now write the 3-sentence summary:"
    )
    try:
        resp = _call_gemini(prompt)
        out = (resp.text or "").strip()
        sents = [s.strip() for s in re.split(r'(?<=[.!?])\s+', out) if s.strip()]
        if not sents:
            return _naive_fallback(text)
        return " ".join(sents[:3])
    except Exception:
        return _naive_fallback(text)

# 4. Feed configuration (durable URLs)
feeds = {
    "https://huggingface.co/blog/feed.xml":      "Hugging Face Blog",
    "https://openai.com/feed.xml?format=xml":    "OpenAI Blog",       # more reliable than /blog/rss.xml
    "https://deepmind.com/blog/feed/basic":      "DeepMind Blog",     # reliable DeepMind feed
    "https://blog.arxiv.org/feed":               "arXiv Blog",
    # Meta AI has no official RSS; consider 3rd-party feed generator if needed.
}

# 5. Helpers for resilient RSS extraction
UA = {"User-Agent": "Mozilla/5.0"}

def extract_entry_link(entry, fallback_html=None):
    """Return a usable link even if <link> is missing."""
    link = (entry.get("link") or "").strip()
    if link:
        return link
    for l in entry.get("links", []) or []:
        href = (l.get("href") or "").strip()
        if href and (l.get("rel") in (None, "", "alternate")):
            return href
    for k in ("id", "guid"):
        cand = (entry.get(k) or "").strip()
        if cand.startswith("http"):
            return cand
    if fallback_html:
        soup = BeautifulSoup(fallback_html, "html.parser")
        a = soup.find("a", href=True)
        if a and a["href"].startswith("http"):
            return a["href"]
    return ""

def extract_pub_dt(entry):
    """Parse published/updated into aware UTC datetimes."""
    st = entry.get("published_parsed") or entry.get("updated_parsed")
    if st:
        return datetime(st.tm_year, st.tm_mon, st.tm_mday, st.tm_hour, st.tm_min, st.tm_sec, tzinfo=timezone.utc)
    raw = (entry.get("published") or entry.get("updated") or "").strip()
    try:
        dt = dateparser.parse(raw)
        if dt and dt.tzinfo is None:
            dt = dt.replace(tzinfo=timezone.utc)
        return dt
    except Exception:
        return datetime(2000, 1, 1, tzinfo=timezone.utc)

# 6. Fetch, dedupe, extract & clean content
all_articles = []
seen_urls = set()
seen_titles_by_source = {}

for url, src in feeds.items():
    try:
        feed = feedparser.parse(url)
    except Exception:
        continue

    for entry in feed.entries[:15]:  # gentle cap per feed
        # Extract raw HTML block early (used for link fallback too)
        raw_html = ""
        try:
            if entry.get("content"):
                raw_html = entry.content[0].value
            elif entry.get("summary"):
                raw_html = entry.summary
            elif entry.get("media_content"):
                maybe_url = entry.media_content[0].get("url", "")
                if maybe_url.startswith("http"):
                    raw_html = requests.get(maybe_url, timeout=10, headers=UA).text
        except Exception:
            raw_html = ""

        link = extract_entry_link(entry, fallback_html=raw_html)
        if not link or link in seen_urls:
            continue

        title = (entry.get("title") or "").strip()
        tl = title.lower()

        lst = seen_titles_by_source.setdefault(src, [])
        if any(difflib.SequenceMatcher(None, tl, other).ratio() > 0.6 for other in lst):
            continue
        lst.append(tl)

        pub_dt = extract_pub_dt(entry)

        # Extract readable content
        content = ""
        if raw_html.strip():
            try:
                clean_html = Document(raw_html).summary()
                content = BeautifulSoup(clean_html, "html.parser").get_text(" ", strip=True)
            except Exception:
                content = ""

        if len(content.split()) < 50:
            try:
                page_html = requests.get(link, timeout=10, headers=UA).text
                clean_page = Document(page_html).summary()
                candidate = BeautifulSoup(clean_page, "html.parser").get_text(" ", strip=True)
                if len(candidate.split()) > len(content.split()):
                    content = candidate
            except Exception:
                pass

        seen_urls.add(link)
        all_articles.append({
            "Source":  src,
            "Date":    pub_dt.strftime("%Y-%m-%d"),
            "Title":   title,
            "Link":    link,
            "Content": content,
            "PubDT":   pub_dt,
        })

# Safety: handle empty result to avoid crashes
if not all_articles:
    print("No articles found; writing empty sheet and exiting.")
    try:
        expected_header = ["Source", "Date", "Title", "Link", "Summary"]
        if not has_header or [c.strip() for c in (ws.row_values(1)[:5] or [])] != expected_header:
            ws.insert_row(expected_header, 1)
        # Do not wipe old content; just exit if nothing new
    except Exception:
        pass
    raise SystemExit(0)

# 7. Select a 5-item batch with ≥3 sources (if possible), prioritize last 48h
MAX_UPDATES_PER_RUN = 5
MIN_DISTINCT_SOURCES = 3
CANDIDATE_POOL = 80  # look across more items so we can diversify

all_articles = sorted(all_articles, key=lambda x: x["PubDT"], reverse=True)

# candidates we haven't posted before (based on Link)
unseen = [a for a in all_articles if a["Link"] not in known][:CANDIDATE_POOL]
if not unseen:
    print("No unseen articles available. Nothing to insert this run.")
    raise SystemExit(0)

distinct_sources_available = len({a["Source"] for a in unseen})
need_distinct = min(MIN_DISTINCT_SOURCES, MAX_UPDATES_PER_RUN, distinct_sources_available)

now_utc = datetime.now(timezone.utc)
recent_cutoff = now_utc - timedelta(days=2)
recent_unseen = [a for a in unseen if a["PubDT"] >= recent_cutoff]
older_unseen  = [a for a in unseen if a["PubDT"] <  recent_cutoff]

selected, used_links, used_sources = [], set(), set()

def pick_batch(candidates, limit, need_distinct, selected, used_links, used_sources):
    for art in candidates:
        if len(selected) >= limit:
            break
        if art["Link"] in used_links:
            continue
        # diversify first
        if len(used_sources) < need_distinct and art["Source"] in used_sources:
            continue
        selected.append(art)
        used_links.add(art["Link"])
        used_sources.add(art["Source"])

# Phase 1: fill from recent window
pick_batch(recent_unseen, MAX_UPDATES_PER_RUN, need_distinct, selected, used_links, used_sources)
# Phase 2: backfill remaining slots by pure recency
if len(selected) < MAX_UPDATES_PER_RUN:
    pick_batch(older_unseen, MAX_UPDATES_PER_RUN, need_distinct, selected, used_links, used_sources)

print(f"Picked {len(selected)} updates; sources in batch: {sorted(used_sources)}")

if not selected:
    print("Nothing selected after filtering; exiting.")
    raise SystemExit(0)

# 8. Summarize only the selected items (reuse when possible)
for i, art in enumerate(selected, start=1):
    print(f"[{i}/{len(selected)}] Summarizing: {art['Title'][:80]}…")
    if art["Link"] in known and known[art["Link"]]:
        art["Summary"] = known[art["Link"]]
        continue
    art["Summary"] = gemini_summary(art["Content"])
    time.sleep(3)  # gentle pacing for free-tier/minute quotas

# 9. Prepend new rows to the sheet (don’t clear existing)
expected_header = ["Source", "Date", "Title", "Link", "Summary"]
try:
    current_header = [c.strip() for c in ws.row_values(1)[:5]]
except Exception:
    current_header = []

# If header missing/mismatched, set it once
if current_header != expected_header:
    ws.insert_row(expected_header, 1)

# Build rows (keep newest first so row 2 is the most recent)
rows_to_insert = [
    [art["Source"], art["Date"], art["Title"], art["Link"], art["Summary"]]
    for art in selected
]

# Insert all at once at row 2 (directly under header)
try:
    ws.insert_rows(rows_to_insert, row=2, value_input_option='RAW')
except TypeError:
    # Fallback for older gspread: insert one-by-one in reverse
    for row in reversed(rows_to_insert):
        ws.insert_row(row, 2, value_input_option='RAW')

print("✅ Prepended updates to Google Sheet without replacing older rows.")
