# -*- coding: utf-8 -*-
"""AI Feed Generator

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/10ZeqUA9uM-b12OGsgKVfeQ0zsJIL5KAK
"""

# 0. Install required libraries


# 1. Imports
import feedparser
import difflib
import requests
import time
from dateutil import parser as dateparser
from datetime import datetime
from bs4 import BeautifulSoup
from readability import Document
import trafilatura
import os
from google.oauth2.service_account import Credentials

import pandas as pd
from concurrent.futures import ThreadPoolExecutor, TimeoutError
from transformers import pipeline

# 2. RSS feeds & human-readable names
feeds = {
    "https://blog.arxiv.org/feed":              "arXiv Blog",
    "https://ai.meta.com/blog/rss/":            "Meta AI Blog",
    "https://huggingface.co/blog/feed.xml":     "Hugging Face Blog",
    "https://openai.com/blog/rss.xml":          "OpenAI Blog",
    "https://deepmind.google/discover/blog/rss": "DeepMind Blog"
}

# 4. Fetch, dedupe, extract & clean content
all_articles = []
seen_urls = set()
seen_titles = []

for url, src in feeds.items():
    feed = feedparser.parse(url)
    for entry in feed.entries:
        # 4a) robust link extraction
        link = (entry.get("link") or entry.get("id") or entry.get("guid", "")).strip()
        if not link or link in seen_urls:
            continue

        title = entry.get("title", "").strip()
        tl = title.lower()
        if any(difflib.SequenceMatcher(None, tl, other).ratio() > 0.6 for other in seen_titles):
            continue

        seen_urls.add(link)
        seen_titles.append(tl)

        # 4b) publication date
        raw_dt = entry.get("published", entry.get("updated", ""))
        try:
            pub_dt = dateparser.parse(raw_dt)
        except:
            pub_dt = datetime(2000, 1, 1)

        # 4c) pull the richest HTML field available
        raw_html = ""
        if entry.get("content"):
            raw_html = entry.content[0].value
        elif entry.get("summary"):
            raw_html = entry.summary
        elif entry.get("media_content"):
            maybe_url = entry.media_content[0].get("url", "")
            if maybe_url.startswith("http"):
                try:
                    raw_html = requests.get(maybe_url, timeout=10).text
                except requests.exceptions.RequestException:
                    raw_html = ""
        raw_html = raw_html or ""

        # 4d) readability-lxml to strip navigation, ads, boilerplate
        content = ""
        if raw_html.strip():
            try:
                clean_html = Document(raw_html).summary()
                content = BeautifulSoup(clean_html, "html.parser") \
                            .get_text(" ", strip=True)
            except Exception:
                content = ""

        # 4e) fallback: fetch full page & re-extract if still tiny (<50 words)
        if len(content.split()) < 50:
            try:
                page_html = requests.get(link, timeout=10).text
                clean_page = Document(page_html).summary()
                candidate = BeautifulSoup(clean_page, "html.parser") \
                              .get_text(" ", strip=True)
                if len(candidate.split()) > len(content.split()):
                    content = candidate
            except Exception:
                pass  # keep whatever content we have

        # 4f) append to list
        all_articles.append({
            "Source":  src,
            "Date":    pub_dt.strftime("%Y-%m-%d"),
            "Title":   title,
            "Link":    link,
            "Content": content,
            "PubDT":   pub_dt
        })

# 5. Keep the 5 newest articles
all_articles = sorted(all_articles, key=lambda x: x["PubDT"], reverse=True)[:5]

# 6. Initialize abstractive summarizer (Pegasus)
summarizer = pipeline(
    "summarization",
    model="google/pegasus-cnn_dailymail",
    tokenizer="google/pegasus-cnn_dailymail"
)

def transformer_summary(text, max_length=120, min_length=30, timeout=60):
    # Skip abstractive summary for very short content
    if len(text.split()) < 50:
        return text
    snippet = text[:1000] + ("…" if len(text) > 1000 else "")
    print("  → Summarizing…", end="", flush=True)
    with ThreadPoolExecutor(max_workers=1) as ex:
        fut = ex.submit(lambda: summarizer(
            snippet,
            max_length=max_length,
            min_length=min_length,
            do_sample=False
        ))
        try:
            out = fut.result(timeout=timeout)
            print(" done.")
            return out[0]["summary_text"].strip()
        except TimeoutError:
            print(" TIMEOUT")
            return snippet
        except Exception as e:
            print(f" ERROR: {e}")
            return snippet

# 7. Generate the final “Summary” field
for i, art in enumerate(all_articles, start=1):
    print(f"[{i}/{len(all_articles)}] Summarizing: {art['Title'][:60]}…")
    art["Summary"] = transformer_summary(art["Content"])
    time.sleep(1)  # rate limit

# 8. Build DataFrame
df = pd.DataFrame(all_articles)[["Source", "Date", "Title", "Link", "Summary"]]

# 9. Write to Google Sheet
sheet_name = "AI Digest Sheet"
try:
    sh = gc.open(sheet_name)
except gspread.SpreadsheetNotFound:
    sh = gc.create(sheet_name)
ws = sh.get_worksheet(0)
ws.clear()
set_with_dataframe(ws, df)

print("✅ AI Digest Sheet updated with cleaner, full-text summaries!")
